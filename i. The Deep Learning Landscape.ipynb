{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e014f489",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "IPython.notebook.set_autosave_interval(15000)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Autosaving every 15 seconds\n"
     ]
    }
   ],
   "source": [
    "%autosave 15"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "544e4bbe",
   "metadata": {},
   "source": [
    "## Stepping Back\n",
    "\n",
    "By now, we know a lot about **Machine Learning**. Let's take a step back and look at the big pitcure:\n",
    "\n",
    "- **Machine Learning** is about **making machines get better at some task by learning from data**, instead of having to explicitly code rules.<br><br>\n",
    "\n",
    "- There are many different types of ML systems: **supervised or not**, **batch or online**, **instance-based or model-based**.<br><br>\n",
    "\n",
    "- In an ML project we gather data in a training set, and we feed the training set to a learning algorithm. If the alogorithm is **model-based**, **it tunes some parameters to fit the model to the training set** (i.e., to make good predictions on the training set itself), and then we hope it will be able to make good predictions on the new cases as well.\n",
    "\n",
    "  If the algorithm is **instance-based** (most trivial form) , **it just learns examples by heart and generalizes to new instances by using a similarity measure** to compare them to the learned instances. For example, a spam filter created this way would just flag all emails that are identical to emails that have already been flagged by users, certainly not the best approach.<br><br>\n",
    "  \n",
    "- The system ain't gonna perform well **if our training set is too small** or **if the data is not representative, is noisy, or is polluted with irrelevant features (garbage in, garbage out)**. Lastly, our model needs to be neither to simple (in whoch case it will underfit) nor too complex (in which case it will overfit)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34865e58",
   "metadata": {},
   "source": [
    "# i. Deep Learning\n",
    "\n",
    "**Deep Learning** is a subset of Machine Learning where effort is being put to make machines learn from vast amounts of data **simulating the way the human brain works**, allowing them to learn and improve from data in a similar way that humans do, all by using **artificial neural networks**.\n",
    "\n",
    "The term **deep learning** refers to **training neural networks**, sometimes very large neural networks.\n",
    "\n",
    "Primarily, deep learning's forte lies in **processing and making predictions on unstructured data** such as images, videos, audio, and natural language text. **It has become particularly useful in recent years due to the explosion of Big Data, where traditional machine learning algorithms may struggle to scale and process such large volumes of data.** \n",
    "\n",
    "\n",
    "### i.i What exactly is a \"neural network\"?\n",
    "\n",
    "- A simplest linear regression model (**RelU function**) just predicting the house price based on number of bedrooms can be considered a **\"neuron\"**, infact it indeed is a neuron. Apparently, a **\"neuron\"** can be thought of as a **simple computational unit that receives one or more inputs, performs a mathematical operation on those inputs, and produces an output**.\n",
    "\n",
    "<img src=\"./media/Screenshot (292).png\" alt=\"Image\" width=\"800\" height=\"600\">\n",
    "\n",
    "- Now, a **\"larger neural network\"** can be obtained by taking and stacking mutltitude of single neurons.\n",
    "\n",
    "<img src=\"./media/Screenshot (293).png\" alt=\"Image\" width=\"800\" height=\"600\">\n",
    "\n",
    "- When we implement a **neural network**, we just need to give it the features and the target variable(s). **Given enough examples with both X and y, neural networks are remarkably good at figuring out functions that accurately map from X to y**. \n",
    "\n",
    "<img src=\"./media/Screenshot (295).png\" alt=\"Image\" width=\"900\" height=\"600\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ddc500b",
   "metadata": {},
   "source": [
    "## i.ii Where Neural Networks are excelling? \n",
    "\n",
    "It turns out that so far, **almost all the economic value created by neural networks has been through one type of machine learning, called supervised learning**.\n",
    "\n",
    "- Possibly the single most lucrative application of deep learning today is **online advertising**, maybe not the most inspiring, but certainly very lucrative, in which, by inputting information about an ad to the website it's thinking of showing you, and some information about the user, **neural networks have gotten very good at predicting whether or not you click on an ad**. And by showing you and showing users the ads that you are most likely to click on, this has been an incredibly lucrative application of neural networks at multiple companies.<br><br>\n",
    "\n",
    "<img src=\"./media/Screenshot (285).png\" alt=\"Image\" width=\"800\" height=\"600\">\n",
    "\n",
    "- For **image applications such as Photo Tagging**, we'll often use **`Convolutional Neural Networks`**, often abbreviated CNN.<br><br>\n",
    "\n",
    "- And for **sequence data**, say for example, audio has a temporal component, right? Audio is played out over time, so **audio is most naturally represented as a \"one-dimensional time series\" or as a \"one-dimensional temporal sequence\"**. \n",
    "\n",
    "  And so for sequence data, we often use an **RNN, a `Recurrent Neural Network`**. \n",
    "  \n",
    "  For **Machine Translation** and stuff, the alphabets or the words come one at a time. So language is also most naturally represented as sequence data. Having said that, somewhat more complex versions of RNNs are often used for these applications.<br><br>\n",
    "  \n",
    "- For more complex applications, like **autonomous driving**, where you have an image, that might suggest more of a CNN, convolution neural network, structure and radar info which is something quite different. **We totally might end up with a more custom, or some more complex, hybrid neural network architecture.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85582d05",
   "metadata": {},
   "source": [
    "<img src=\"./media/Screenshot (284).png\" alt=\"Image\" width=\"800\" height=\"600\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6e98c2f",
   "metadata": {},
   "source": [
    "## i.iii Why Deep learning is taking off suddenly so well now?\n",
    "\n",
    "Despite being there for nearly a decade, why deep learning is doing so well now only? That's because **scale has been driving the deep learning progress**.<br><br>\n",
    "\n",
    "\n",
    "**1.** Performance of Machine Learning algorithms plateaus improves with increase in data **but overtime it pretty much plateaus as if they don't know what do this with huge amount of data**.<br><br>\n",
    "\n",
    "**2.** Over the last 20 years, **we have so much so data now than traditional algorthims are able to effectively take advantage of.**<br><br> \n",
    "\n",
    "**3.** There are primarily three factors that determine this progress, one's to have **large enough neural network**, other one is to throw **huge amounts of \"labelled\" data** over it, and lastly **the fucking computing power required to process that data**.\n",
    "\n",
    "    However, this also works only up to a certain point because there would be a time when we'd run outta data or have that much of a bigger neural network which would take close to eternity to train."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "354a0da8",
   "metadata": {},
   "source": [
    "<img src=\"./media/Screenshot (288).png\" alt=\"Image\" width=\"900\" height=\"500\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "556b3de3",
   "metadata": {},
   "source": [
    "**4.** Over the left regime of the above figure, where we do have smaller training sets, **the relative ordering of the algorithms is not very well defined** meaning that it is not always clear which algorithm will perform the best for a given problem. In other words, the performance of different algorithms may be similar or even interchangeable, depending on the specifics of the problem and the dataset. \n",
    "\n",
    "Thus, if we don't have required enough **labelled training data**, it is often up to our skill at hand **engineering features** that determines the success.\n",
    "\n",
    "So in such cases, someone motivated enough implementing traditional algorithms can prevail provided they've got **better feature engineering skills**, than someone implementing neural networks just for the sake of it.<br><br>\n",
    "\n",
    "**5.** **Algorithmic Innovation:** Interestingly many **algorithmic innovations** happen about tryna make neural networks learn much faster. One of the huge breakthrough demonstrating this was switching from **Sigmoid Activation function** to **RelU (Rectified Linear Units)**.\n",
    "\n",
    "One of the problems of using sigmoid function in machine learning is that **there are these regions where the slope of the function where the gradient is nearly zero** and **so learning becomes really slow**, because when we implement gradient descent and gradient is zero the parameters change very slowly. By switching over to **RELU**, the gradient is equal to 1 for all positive values of input. **The impact of this algorithmic innovation was it really helped computation.**\n",
    "\n",
    "<img src=\"./media/Screenshot (289).png\" alt=\"Image\" width=\"900\" height=\"500\">\n",
    "\n",
    "So innovations like these bring about **\"faster computation\"** which in turn helps in terms of **speeding up the rate at which we can get an experimental result back** and this has really helped both practitioners of neural networks as well as researchers working and deep learning iterate much faster and improve the ideas much faster."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
